{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample,MelSpectrogram\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
    "    It will drop the last few seconds of a very small portion of the utterances.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        \n",
    "        assert sample_rate == 16000\n",
    "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
    "        \n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return (mel, text)\n",
    "    \n",
    "\n",
    "# 예시 사용\n",
    "# dataset = MyAudioDataset(device='cuda')\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "# for waveform, filename in loader:\n",
    "#     print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dataset = LibriSpeech(\"test-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory=\"bigdata\", device='cpu', sample_rate=16000, n_mels=80):\n",
    "        self.directory = directory\n",
    "        self.device = device\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.filenames = [f for f in os.listdir(directory) if f.endswith('.mp3')]\n",
    "        self.mel_transformer = MelSpectrogram(sample_rate=self.sample_rate, n_mels=self.n_mels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        path = os.path.join(self.directory, filename)\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        \n",
    "        # 오디오를 모노로 변환\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # 리샘플링\n",
    "        if sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "#         # 멜 스펙트로그램 변환\n",
    "#         mel = self.mel_transformer(waveform)\n",
    "\n",
    "        audio = whisper.pad_or_trim(waveform.flatten()).to(self.device)\n",
    "        \n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "\n",
    "        return mel, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyAudioDataset(directory='./bigdata/processed_audio', device='cuda:0')\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is English-only and has 71,825,408 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base.en\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf16ed8bf774acd9ad86630eb005f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for mels, texts in tqdm(loader):\n",
    "    results = model.decode(mels, options)\n",
    "    hypotheses.extend([result.text for result in results])\n",
    "    references.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Officer, to come at 6 today with a tight drive...</td>\n",
       "      <td>call_1_1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59-13, carefully outside unit, and you have th...</td>\n",
       "      <td>call_1_2.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let me film you 980-9930</td>\n",
       "      <td>call_1_3.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the column over side drive 15 A-13 or her neck.</td>\n",
       "      <td>call_1_4.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So,</td>\n",
       "      <td>call_1_5.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23151</th>\n",
       "      <td>I'm going to get a right. Get out of the house...</td>\n",
       "      <td>call_99_37.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23152</th>\n",
       "      <td>I'm going to get out of that house, man. Okay,...</td>\n",
       "      <td>call_99_38.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23153</th>\n",
       "      <td>I'm going to need to get out of there.</td>\n",
       "      <td>call_99_39.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23154</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>call_99_40.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23155</th>\n",
       "      <td>Thank you.</td>\n",
       "      <td>call_99_41.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              hypothesis       reference\n",
       "0      Officer, to come at 6 today with a tight drive...    call_1_1.mp3\n",
       "1      59-13, carefully outside unit, and you have th...    call_1_2.mp3\n",
       "2                               Let me film you 980-9930    call_1_3.mp3\n",
       "3        the column over side drive 15 A-13 or her neck.    call_1_4.mp3\n",
       "4                                                    So,    call_1_5.mp3\n",
       "...                                                  ...             ...\n",
       "23151  I'm going to get a right. Get out of the house...  call_99_37.mp3\n",
       "23152  I'm going to get out of that house, man. Okay,...  call_99_38.mp3\n",
       "23153             I'm going to need to get out of there.  call_99_39.mp3\n",
       "23154                                              Okay.  call_99_40.mp3\n",
       "23155                                         Thank you.  call_99_41.mp3\n",
       "\n",
       "[23156 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./bigdata/result/15sec_whisper.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jiwer\n",
    "# from whisper.normalizers import EnglishTextNormalizer\n",
    "\n",
    "# normalizer = EnglishTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\n",
    "# data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "# data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "\n",
    "# print(f\"WER: {wer * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311llama2",
   "language": "python",
   "name": "py311llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
