{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad38466fa01c40878584f74dc2b163a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피보나치 수열(PiBoNA치 수열)은 수학에서 유명한 수열 중 하나로, 피보나치 수열은 1, 1, 2, 3, 2, 3, 4, 5, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610,... 로 시작하는 수열입니다. 피보나치 수열은 수학자 막스 프리쉬(Maksimilian von Koch의 이름에서 따왔으며, 1882년부터 1901년까지 수학자들이 연구하여 성립한 수열입니다.\n",
      "\n",
      "피보나치 수열을 파이썬 코드로 구현하려면, 먼저 수열을 생성하는 함수를 정의해야 하겠습니다. 예를 들어, 이 수열을 생성하는 함수를 정의하려면 다음과 같은 파이썬 코드를 작성할 수 있습니다.\n",
      "```python\n",
      "def fibonacci(n):  return [1, 1] if n == 1 1 elif n == 2 return [1, 2] else return [a, a + b] for a, b in fib 이전 결과에 a, b를 더하는 방식으로 작성할 수 있습니다. 이 코드는 피보나치 수열을 생성하는 함수를 정의합니다.\n",
      "\n",
      "이 코드를 실행하면, 입력된 숫자 n에 따라 피보나치 수열의 해당 숫자를 반환합니다.\n",
      "```\n",
      "이 코드를 실행하면, 입력된 숫자 n에 따라 피보나치 수열의 해당 숫자를 반환합니다. 예를 들어, fib(5)는 8을 반환합니다. 이제 이 함수를 사용하여 피보나치 수열을 생성하면 됩니다.\n",
      "\n",
      "이 코드를 실행하면, 피보나치 수열을 생성합니다. 예를 들어, print(fib(10))하면, [1, 1, 2, 3, 2, 3, 5, 5, 8]을 출력합니다. 이제 피보나치 수열을 더 생성하면 됩니다.\n",
      "\n",
      "이 코드는 피보나치 수열을 생성하는 함수를 정의하여 피보나치 수열을 생성할 수 있습니다. 이 코드를 실행하면, 피보나치 수열을 생성할 수 있습니다. 이제 피보나치 수열을 사용하여 다양한\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3,4' # nvidia-smi로 비어있는 gpu 확인하고 여기서 선택할것!\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"saltlux/Ko-Llama3-Luxia-8B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"피보나치 수열이 뭐야? 그리고 피보나치 수열에 대해 파이썬 코드를 짜줘볼래?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=512,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=1,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피보나치 수열(피보나치의 수열)은 수학에서 자연수열의 한 예로, 피보나치 수열은 0, 1, 1, 2, 3, 2, 5, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 985, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 28657, 47032, 83240, 13933, 25856, 4189, 65651, 104623, 17711, 286\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"피보나치 수열이 뭐야? 그리고 피보나치 수열에 대해 파이썬 코드를 짜줘볼래?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         eos_token_id=terminators,\n",
    "#     #     do_sample=True,\n",
    "#     #     temperature=1,\n",
    "#     #     top_p=0.9,\n",
    "#     )\n",
    "# response = outputs[0][input_ids.shape[-1]:]\n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271, 108200, 104834,  24486, 102783,\n",
      "            245, 103850,    229,  43139,  27796, 126761, 101482,  21028, 127296,\n",
      "          19954,  82273, 113760, 123154, 101360, 108280, 104834, 102893, 108386,\n",
      "         124295,     13, 107036, 126958,  34804, 104008,  32179,  17155,  46295,\n",
      "              8,  43139, 126958,  34983,  59269,    246,     13, 128009, 128006,\n",
      "            882, 128007,    271, 102477,  42771,  61415,  60798,  29833,  55055,\n",
      "          13094, 113792,  90759,     30, 107536, 104064,  42771,  61415,  60798,\n",
      "          29833,  55055,  19954, 112107,  56069,  13094,    168,    235,    105,\n",
      "          92705,  18918,  49011,    250,  59269,    246, 113110,  54542,     30,\n",
      "         128009, 128006,  78191, 128007,    271]], device='cuda:0')\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "피보나치 수열이 뭐야? 그리고 피보나치 수열에 대해 파이썬 코드를 짜줘볼래?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"피보나치 수열이 뭐야? 그리고 피보나치 수열에 대해 파이썬 코드를 짜줘볼래?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# print(input_ids)\n",
    "# print(tokenizer.decode(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>안녕하세요.\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.decode(tokenizer(\"안녕하세요.\")[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"트리와 그래프의 차이가 뭐야?\"},\n",
    "# ]\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         eos_token_id=terminators,\n",
    "#     #     do_sample=True,\n",
    "#     #     temperature=1,\n",
    "#     #     top_p=0.9,\n",
    "#     )\n",
    "# response = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "트리와 그래프의 차이가 뭐야?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "트리와 그래프는 둘 다 데이터를 표현하는 도구이지만, 그 구조와 기능이 다릅니다. 트리는 계층적 구조를 가지고 있는 반면, 그래프는 노드와 에지로 구성된 네트워크 구조를 가집니다.\n",
      "\n",
      "트리는 계층적 구조를 가지고 있기 때문에, 데이터를 계층적으로 분류하거나 정렬하는 데 적합합니다. 예를 들어, 파일 시스템의 디렉터리 구조나 조직의 조직 구조를 표현하는 데 사용됩니다. 반면, 그래프는 노드와 에지를 통해 데이터 간의 관계를 표현하는 데 적합합니다. 예를 들어, 소셜 네트워크의 구조나 웹 페이지의 링크 구조를 표현하는 데 사용됩니다. 트리는 계층적 구조를 가지고 있기 때문에, 데이터를 쉽게 검색하거나 정렬할 수 있습니다. 반면, 그래프는 데이터 간의 관계를 쉽게 찾을 수 있습니다.\n",
      "\n",
      "따라서, 트리는 계층적 구조를 필요로 하는 데이터를 표현하는 데 적합하고, 그래프는 관계를 필요로 하는 데이터를 표현하는 데 적합합니다. 이 두 가지 도구를 적절하게 사용하면, 데이터를 더 잘 표현할 수 있습니다.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.decode(response, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3,4' # nvidia-smi로 비어있는 gpu 확인하고 여기서 선택할것!\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer,DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B-Instruct-preview\" \n",
    "dataset = load_dataset(\"jojo0217/korean_safe_conversation\",split=\"train\")\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"kollama3-safeconv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '친구들과 여행을 가보신 적이 있나요? 가봤다면 어디로 가보셨나요?',\n",
       " 'output': '제가 여행을 가거나 경험을 할 수는 없는 인공 지능 모델이므로, 실제로 여행을 간 적이 없습니다. 하지만 여행은 새로운 장소를 탐험하고 다양한 문화를 경험하는 멋진 활동 중 하나입니다. 여행은 많은 사람들에게 흥미로운 경험과 추억을 제공합니다. 여행에 대해 어떤 정보나 추천이 필요하시다면, 언제든 도움을 드릴 수 있습니다.',\n",
       " 'input': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0233c4e91cf43ffa5f99929cc235969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "#     with open('sample.txt', 'w') as file:\n",
    "#         file.write(example)  # 각 키를 새로운 줄에\n",
    "    for input_msg,output_msg in zip(example['instruction'],example['output']):\n",
    "        text =f'''<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{output_msg}<|eot_id|>'''\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# indices = list(range(5))\n",
    "\n",
    "# # select 메서드를 사용하여 새로운 Dataset 객체 생성\n",
    "# small_dataset = dataset.select(indices)\n",
    "\n",
    "\n",
    "#print(type(small_dataset))\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    #dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53958' max='53958' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53958/53958 11:22:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.583200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>1.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.532200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>1.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>1.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>1.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>1.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>1.502400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>1.492700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>1.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>1.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>1.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>1.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>1.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>1.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.511700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=53958, training_loss=1.552601764534308, metrics={'train_runtime': 40976.21, 'train_samples_per_second': 1.317, 'train_steps_per_second': 1.317, 'total_flos': 6.942659289028854e+17, 'train_loss': 1.552601764534308, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kollama3-safeconv/tokenizer_config.json',\n",
       " 'kollama3-safeconv/special_tokens_map.json',\n",
       " 'kollama3-safeconv/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Who is Leonardo Da Vinci?\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(new_model    ,quantization_config=quant_config,\n",
    "    device_map=\"auto\")\n",
    "# 필요하다면, 여기서 수동으로 양자화 과정을 수행\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = formatting_prompts_func(dataset)\n",
    "new_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "#     with open('sample.txt', 'w') as file:\n",
    "#         file.write(example)  # 각 키를 새로운 줄에 쓰기\n",
    "    for input_msg,output_msg in zip(example['instruction'],example['output']):\n",
    "        text =f'''<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "친절한 챗봇으로서 상대방의 요청에 최대한 자세하고 친절하게 답하자. 모든 대답은 한국어(Korean)으로 대답해줘.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = formatting_prompts_func(dataset)\n",
    "new_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids = tokenizer.encode(new_ds[0],return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "    #     do_sample=True,\n",
    "    #     temperature=1,\n",
    "    #     top_p=0.9,\n",
    "    )\n",
    "response = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(response, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311llama2",
   "language": "python",
   "name": "py311llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
